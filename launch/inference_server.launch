<?xml version="1.0" encoding="UTF-8"?>

<launch>
	<!-- model_name defines the name of the model that you want to run inference on-->
	<arg name="model_name" default="ssd_inception_v2_coco_11_06_2017"/>
	<!-- tf_models_path defines the path to the folder where the TensorFlow Object Detection API is located-->
	<arg name="tf_models_path" default="/TFM/models"/>
	<!-- model_database_path defines the path in which all the object detection models are present-->
	<arg name="model_database_path" default="/TFM/model_database"/> 

	<node name="inference_server" pkg="inference_server" type="inference_server_node.py" required="true" output="screen">
		<param name="model_name" value="$(arg model_name)"/>
		<param name="tf_models_path" value="$(arg tf_models_path)"/>
		<param name="model_database_path" value="$(arg model_database_path)"/>
		<rosparam param="desired_classes">['all']</rosparam>
	</node>
</launch>
